2025-09-27 17:23:28,555 [trainer.py] => dataset: cifar100
2025-09-27 17:23:28,555 [trainer.py] => smart_defaults: True
2025-09-27 17:23:28,555 [trainer.py] => user: authors
2025-09-27 17:23:28,555 [trainer.py] => test: True
2025-09-27 17:23:28,555 [trainer.py] => memory_size: 0
2025-09-27 17:23:28,555 [trainer.py] => memory_per_class: 0
2025-09-27 17:23:28,555 [trainer.py] => fixed_memory: False
2025-09-27 17:23:28,556 [trainer.py] => shuffle: True
2025-09-27 17:23:28,556 [trainer.py] => init_cls: 20
2025-09-27 17:23:28,556 [trainer.py] => increment: 20
2025-09-27 17:23:28,556 [trainer.py] => model_name: sldc
2025-09-27 17:23:28,556 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:23:28,556 [trainer.py] => weight_decay: 0.0
2025-09-27 17:23:28,556 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:23:28,556 [trainer.py] => sce_a: 0.5
2025-09-27 17:23:28,556 [trainer.py] => sce_b: 0.5
2025-09-27 17:23:28,556 [trainer.py] => seed_list: [1993]
2025-09-27 17:23:28,557 [trainer.py] => iterations: 1500
2025-09-27 17:23:28,557 [trainer.py] => warmup_steps: 200
2025-09-27 17:23:28,557 [trainer.py] => ca_epochs: 5
2025-09-27 17:23:28,557 [trainer.py] => optimizer: adamw
2025-09-27 17:23:28,557 [trainer.py] => lrate: 0.0005
2025-09-27 17:23:28,557 [trainer.py] => head_scale: 1.0
2025-09-27 17:23:28,557 [trainer.py] => batch_size: 24
2025-09-27 17:23:28,557 [trainer.py] => evaluate_final_only: True
2025-09-27 17:23:28,557 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:23:28,557 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:23:28,557 [trainer.py] => kd_type: feat
2025-09-27 17:23:28,557 [trainer.py] => alpha_t: 1.0
2025-09-27 17:23:28,557 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:23:28,557 [trainer.py] => compensate: True
2025-09-27 17:23:28,557 [trainer.py] => eval_only: True
2025-09-27 17:23:28,557 [trainer.py] => lora_rank: 4
2025-09-27 17:23:28,557 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:23:28,557 [trainer.py] => weight_temp: 5.0
2025-09-27 17:23:28,557 [trainer.py] => weight_kind: log1p
2025-09-27 17:23:28,557 [trainer.py] => weight_p: 1.0
2025-09-27 17:23:28,557 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:23:28,557 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:23:28,557 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:23:28,557 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:23:28,557 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:23:28,557 [trainer.py] => l2_protection: True
2025-09-27 17:23:28,557 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:23:28,557 [trainer.py] => seed: 1993
2025-09-27 17:23:28,557 [trainer.py] => run_id: 0
2025-09-27 17:23:28,557 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:24:18,275 [trainer.py] => dataset: cifar100
2025-09-27 17:24:18,276 [trainer.py] => smart_defaults: True
2025-09-27 17:24:18,276 [trainer.py] => user: authors
2025-09-27 17:24:18,276 [trainer.py] => test: True
2025-09-27 17:24:18,276 [trainer.py] => memory_size: 0
2025-09-27 17:24:18,276 [trainer.py] => memory_per_class: 0
2025-09-27 17:24:18,276 [trainer.py] => fixed_memory: False
2025-09-27 17:24:18,276 [trainer.py] => shuffle: True
2025-09-27 17:24:18,276 [trainer.py] => init_cls: 20
2025-09-27 17:24:18,277 [trainer.py] => increment: 20
2025-09-27 17:24:18,277 [trainer.py] => model_name: sldc
2025-09-27 17:24:18,277 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:24:18,277 [trainer.py] => weight_decay: 0.0
2025-09-27 17:24:18,277 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:24:18,277 [trainer.py] => sce_a: 0.5
2025-09-27 17:24:18,277 [trainer.py] => sce_b: 0.5
2025-09-27 17:24:18,277 [trainer.py] => seed_list: [1993]
2025-09-27 17:24:18,277 [trainer.py] => iterations: 1500
2025-09-27 17:24:18,278 [trainer.py] => warmup_steps: 200
2025-09-27 17:24:18,278 [trainer.py] => ca_epochs: 5
2025-09-27 17:24:18,278 [trainer.py] => optimizer: adamw
2025-09-27 17:24:18,278 [trainer.py] => lrate: 0.0005
2025-09-27 17:24:18,278 [trainer.py] => head_scale: 1.0
2025-09-27 17:24:18,278 [trainer.py] => batch_size: 24
2025-09-27 17:24:18,278 [trainer.py] => evaluate_final_only: True
2025-09-27 17:24:18,278 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:24:18,278 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:24:18,278 [trainer.py] => kd_type: feat
2025-09-27 17:24:18,278 [trainer.py] => alpha_t: 1.0
2025-09-27 17:24:18,278 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:24:18,278 [trainer.py] => compensate: True
2025-09-27 17:24:18,279 [trainer.py] => eval_only: True
2025-09-27 17:24:18,279 [trainer.py] => lora_rank: 4
2025-09-27 17:24:18,279 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:24:18,279 [trainer.py] => weight_temp: 5.0
2025-09-27 17:24:18,279 [trainer.py] => weight_kind: log1p
2025-09-27 17:24:18,279 [trainer.py] => weight_p: 1.0
2025-09-27 17:24:18,279 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:24:18,279 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:24:18,279 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:24:18,279 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:24:18,279 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:24:18,279 [trainer.py] => l2_protection: True
2025-09-27 17:24:18,279 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:24:18,279 [trainer.py] => seed: 1993
2025-09-27 17:24:18,279 [trainer.py] => run_id: 0
2025-09-27 17:24:18,280 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:24:19,286 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:24:19,286 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:24:19,292 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:24:19,776 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:24:20,230 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:24:28,644 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:24:28,644 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:24:28,645 [trainer.py] => All params: 86469888
2025-09-27 17:24:28,646 [trainer.py] => Trainable params: 672768
2025-09-27 17:27:03,760 [trainer.py] => dataset: cifar100
2025-09-27 17:27:03,760 [trainer.py] => smart_defaults: True
2025-09-27 17:27:03,761 [trainer.py] => user: authors
2025-09-27 17:27:03,761 [trainer.py] => test: True
2025-09-27 17:27:03,761 [trainer.py] => memory_size: 0
2025-09-27 17:27:03,761 [trainer.py] => memory_per_class: 0
2025-09-27 17:27:03,761 [trainer.py] => fixed_memory: False
2025-09-27 17:27:03,761 [trainer.py] => shuffle: True
2025-09-27 17:27:03,761 [trainer.py] => init_cls: 20
2025-09-27 17:27:03,761 [trainer.py] => increment: 20
2025-09-27 17:27:03,761 [trainer.py] => model_name: sldc
2025-09-27 17:27:03,761 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:27:03,761 [trainer.py] => weight_decay: 0.0
2025-09-27 17:27:03,761 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:27:03,761 [trainer.py] => sce_a: 0.5
2025-09-27 17:27:03,761 [trainer.py] => sce_b: 0.5
2025-09-27 17:27:03,761 [trainer.py] => seed_list: [1993]
2025-09-27 17:27:03,762 [trainer.py] => iterations: 1500
2025-09-27 17:27:03,762 [trainer.py] => warmup_steps: 200
2025-09-27 17:27:03,762 [trainer.py] => ca_epochs: 5
2025-09-27 17:27:03,762 [trainer.py] => optimizer: adamw
2025-09-27 17:27:03,762 [trainer.py] => lrate: 0.0005
2025-09-27 17:27:03,762 [trainer.py] => head_scale: 1.0
2025-09-27 17:27:03,762 [trainer.py] => batch_size: 24
2025-09-27 17:27:03,762 [trainer.py] => evaluate_final_only: True
2025-09-27 17:27:03,762 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:27:03,762 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:27:03,762 [trainer.py] => kd_type: feat
2025-09-27 17:27:03,762 [trainer.py] => alpha_t: 1.0
2025-09-27 17:27:03,762 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:27:03,762 [trainer.py] => compensate: True
2025-09-27 17:27:03,763 [trainer.py] => eval_only: True
2025-09-27 17:27:03,763 [trainer.py] => lora_rank: 4
2025-09-27 17:27:03,763 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:27:03,763 [trainer.py] => weight_temp: 5.0
2025-09-27 17:27:03,763 [trainer.py] => weight_kind: log1p
2025-09-27 17:27:03,763 [trainer.py] => weight_p: 1.0
2025-09-27 17:27:03,763 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:27:03,763 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:27:03,763 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:27:03,763 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:27:03,763 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:27:03,763 [trainer.py] => l2_protection: True
2025-09-27 17:27:03,763 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:27:03,763 [trainer.py] => seed: 1993
2025-09-27 17:27:03,763 [trainer.py] => run_id: 0
2025-09-27 17:27:03,763 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:27:04,798 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:27:04,800 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:27:04,806 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:27:05,272 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:27:05,710 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:27:13,488 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:27:13,489 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:27:13,489 [trainer.py] => All params: 86469888
2025-09-27 17:27:13,490 [trainer.py] => Trainable params: 672768
2025-09-27 17:27:13,937 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 17:28:37,953 [trainer.py] => dataset: cifar100
2025-09-27 17:28:37,953 [trainer.py] => smart_defaults: True
2025-09-27 17:28:37,953 [trainer.py] => user: authors
2025-09-27 17:28:37,954 [trainer.py] => test: True
2025-09-27 17:28:37,954 [trainer.py] => memory_size: 0
2025-09-27 17:28:37,954 [trainer.py] => memory_per_class: 0
2025-09-27 17:28:37,954 [trainer.py] => fixed_memory: False
2025-09-27 17:28:37,954 [trainer.py] => shuffle: True
2025-09-27 17:28:37,954 [trainer.py] => init_cls: 20
2025-09-27 17:28:37,954 [trainer.py] => increment: 20
2025-09-27 17:28:37,954 [trainer.py] => model_name: sldc
2025-09-27 17:28:37,955 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:28:37,955 [trainer.py] => weight_decay: 0.0
2025-09-27 17:28:37,955 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:28:37,955 [trainer.py] => sce_a: 0.5
2025-09-27 17:28:37,955 [trainer.py] => sce_b: 0.5
2025-09-27 17:28:37,955 [trainer.py] => seed_list: [1993]
2025-09-27 17:28:37,955 [trainer.py] => iterations: 1500
2025-09-27 17:28:37,955 [trainer.py] => warmup_steps: 200
2025-09-27 17:28:37,955 [trainer.py] => ca_epochs: 5
2025-09-27 17:28:37,955 [trainer.py] => optimizer: adamw
2025-09-27 17:28:37,955 [trainer.py] => lrate: 0.0005
2025-09-27 17:28:37,955 [trainer.py] => head_scale: 1.0
2025-09-27 17:28:37,955 [trainer.py] => batch_size: 24
2025-09-27 17:28:37,955 [trainer.py] => evaluate_final_only: True
2025-09-27 17:28:37,955 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:28:37,955 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:28:37,955 [trainer.py] => kd_type: feat
2025-09-27 17:28:37,955 [trainer.py] => alpha_t: 1.0
2025-09-27 17:28:37,955 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:28:37,955 [trainer.py] => compensate: True
2025-09-27 17:28:37,955 [trainer.py] => eval_only: False
2025-09-27 17:28:37,955 [trainer.py] => lora_rank: 4
2025-09-27 17:28:37,955 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:28:37,955 [trainer.py] => weight_temp: 5.0
2025-09-27 17:28:37,955 [trainer.py] => weight_kind: log1p
2025-09-27 17:28:37,955 [trainer.py] => weight_p: 1.0
2025-09-27 17:28:37,955 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:28:37,955 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:28:37,955 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:28:37,955 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:28:37,955 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:28:37,957 [trainer.py] => l2_protection: True
2025-09-27 17:28:37,957 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:28:37,957 [trainer.py] => seed: 1993
2025-09-27 17:28:37,957 [trainer.py] => run_id: 0
2025-09-27 17:28:37,957 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:28:38,988 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:28:38,989 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:28:38,996 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:28:39,462 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:28:39,946 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:28:47,547 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:28:47,547 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:28:47,548 [trainer.py] => All params: 86469888
2025-09-27 17:28:47,548 [trainer.py] => Trainable params: 672768
2025-09-27 17:28:47,998 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 17:31:41,533 [trainer.py] => dataset: cifar100
2025-09-27 17:31:41,533 [trainer.py] => smart_defaults: True
2025-09-27 17:31:41,533 [trainer.py] => user: authors
2025-09-27 17:31:41,533 [trainer.py] => test: True
2025-09-27 17:31:41,533 [trainer.py] => memory_size: 0
2025-09-27 17:31:41,534 [trainer.py] => memory_per_class: 0
2025-09-27 17:31:41,534 [trainer.py] => fixed_memory: False
2025-09-27 17:31:41,534 [trainer.py] => shuffle: True
2025-09-27 17:31:41,534 [trainer.py] => init_cls: 20
2025-09-27 17:31:41,534 [trainer.py] => increment: 20
2025-09-27 17:31:41,534 [trainer.py] => model_name: sldc
2025-09-27 17:31:41,534 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:31:41,534 [trainer.py] => weight_decay: 0.0
2025-09-27 17:31:41,534 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:31:41,534 [trainer.py] => sce_a: 0.5
2025-09-27 17:31:41,534 [trainer.py] => sce_b: 0.5
2025-09-27 17:31:41,535 [trainer.py] => seed_list: [1993]
2025-09-27 17:31:41,535 [trainer.py] => iterations: 1500
2025-09-27 17:31:41,535 [trainer.py] => warmup_steps: 200
2025-09-27 17:31:41,535 [trainer.py] => ca_epochs: 5
2025-09-27 17:31:41,535 [trainer.py] => optimizer: adamw
2025-09-27 17:31:41,535 [trainer.py] => lrate: 0.0005
2025-09-27 17:31:41,535 [trainer.py] => head_scale: 1.0
2025-09-27 17:31:41,535 [trainer.py] => batch_size: 24
2025-09-27 17:31:41,535 [trainer.py] => evaluate_final_only: True
2025-09-27 17:31:41,535 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:31:41,536 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:31:41,536 [trainer.py] => kd_type: feat
2025-09-27 17:31:41,536 [trainer.py] => alpha_t: 1.0
2025-09-27 17:31:41,536 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:31:41,536 [trainer.py] => compensate: True
2025-09-27 17:31:41,536 [trainer.py] => eval_only: False
2025-09-27 17:31:41,536 [trainer.py] => lora_rank: 4
2025-09-27 17:31:41,536 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:31:41,536 [trainer.py] => weight_temp: 5.0
2025-09-27 17:31:41,536 [trainer.py] => weight_kind: log1p
2025-09-27 17:31:41,536 [trainer.py] => weight_p: 1.0
2025-09-27 17:31:41,536 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:31:41,536 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:31:41,536 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:31:41,536 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:31:41,537 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:31:41,537 [trainer.py] => l2_protection: True
2025-09-27 17:31:41,537 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:31:41,537 [trainer.py] => seed: 1993
2025-09-27 17:31:41,537 [trainer.py] => run_id: 0
2025-09-27 17:31:41,537 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:31:42,581 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:31:42,582 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:31:42,588 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:31:43,054 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:31:43,480 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:31:51,059 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:31:51,060 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:31:51,060 [trainer.py] => All params: 86469888
2025-09-27 17:31:51,061 [trainer.py] => Trainable params: 672768
2025-09-27 17:31:51,510 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 17:46:00,539 [trainer.py] => dataset: cifar100
2025-09-27 17:46:00,539 [trainer.py] => smart_defaults: True
2025-09-27 17:46:00,540 [trainer.py] => user: authors
2025-09-27 17:46:00,540 [trainer.py] => test: True
2025-09-27 17:46:00,540 [trainer.py] => memory_size: 0
2025-09-27 17:46:00,540 [trainer.py] => memory_per_class: 0
2025-09-27 17:46:00,540 [trainer.py] => fixed_memory: False
2025-09-27 17:46:00,540 [trainer.py] => shuffle: True
2025-09-27 17:46:00,540 [trainer.py] => init_cls: 20
2025-09-27 17:46:00,540 [trainer.py] => increment: 20
2025-09-27 17:46:00,540 [trainer.py] => model_name: sldc
2025-09-27 17:46:00,540 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:46:00,540 [trainer.py] => weight_decay: 0.0
2025-09-27 17:46:00,540 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:46:00,540 [trainer.py] => sce_a: 0.5
2025-09-27 17:46:00,540 [trainer.py] => sce_b: 0.5
2025-09-27 17:46:00,540 [trainer.py] => seed_list: [1993]
2025-09-27 17:46:00,540 [trainer.py] => iterations: 1500
2025-09-27 17:46:00,540 [trainer.py] => warmup_steps: 200
2025-09-27 17:46:00,541 [trainer.py] => ca_epochs: 5
2025-09-27 17:46:00,541 [trainer.py] => optimizer: adamw
2025-09-27 17:46:00,541 [trainer.py] => lrate: 0.0005
2025-09-27 17:46:00,541 [trainer.py] => head_scale: 1.0
2025-09-27 17:46:00,541 [trainer.py] => batch_size: 24
2025-09-27 17:46:00,541 [trainer.py] => evaluate_final_only: True
2025-09-27 17:46:00,541 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:46:00,541 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:46:00,541 [trainer.py] => kd_type: feat
2025-09-27 17:46:00,541 [trainer.py] => alpha_t: 1.0
2025-09-27 17:46:00,541 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:46:00,541 [trainer.py] => compensate: True
2025-09-27 17:46:00,541 [trainer.py] => eval_only: False
2025-09-27 17:46:00,541 [trainer.py] => lora_rank: 4
2025-09-27 17:46:00,541 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:46:00,542 [trainer.py] => weight_temp: 5.0
2025-09-27 17:46:00,542 [trainer.py] => weight_kind: log1p
2025-09-27 17:46:00,542 [trainer.py] => weight_p: 1.0
2025-09-27 17:46:00,542 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:46:00,542 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:46:00,542 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:46:00,542 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:46:00,542 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:46:00,542 [trainer.py] => l2_protection: True
2025-09-27 17:46:00,543 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:46:00,543 [trainer.py] => seed: 1993
2025-09-27 17:46:00,543 [trainer.py] => run_id: 0
2025-09-27 17:46:00,543 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:46:01,596 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:46:01,597 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:46:01,603 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:46:02,070 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:46:02,509 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:46:10,329 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:46:10,329 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:46:10,330 [trainer.py] => All params: 86469888
2025-09-27 17:46:10,330 [trainer.py] => Trainable params: 672768
2025-09-27 17:46:47,116 [trainer.py] => dataset: cifar100
2025-09-27 17:46:47,116 [trainer.py] => smart_defaults: True
2025-09-27 17:46:47,116 [trainer.py] => user: authors
2025-09-27 17:46:47,116 [trainer.py] => test: True
2025-09-27 17:46:47,116 [trainer.py] => memory_size: 0
2025-09-27 17:46:47,117 [trainer.py] => memory_per_class: 0
2025-09-27 17:46:47,117 [trainer.py] => fixed_memory: False
2025-09-27 17:46:47,117 [trainer.py] => shuffle: True
2025-09-27 17:46:47,117 [trainer.py] => init_cls: 20
2025-09-27 17:46:47,117 [trainer.py] => increment: 20
2025-09-27 17:46:47,117 [trainer.py] => model_name: sldc
2025-09-27 17:46:47,117 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:46:47,117 [trainer.py] => weight_decay: 0.0
2025-09-27 17:46:47,117 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:46:47,117 [trainer.py] => sce_a: 0.5
2025-09-27 17:46:47,117 [trainer.py] => sce_b: 0.5
2025-09-27 17:46:47,117 [trainer.py] => seed_list: [1993]
2025-09-27 17:46:47,117 [trainer.py] => iterations: 1500
2025-09-27 17:46:47,117 [trainer.py] => warmup_steps: 200
2025-09-27 17:46:47,117 [trainer.py] => ca_epochs: 5
2025-09-27 17:46:47,118 [trainer.py] => optimizer: adamw
2025-09-27 17:46:47,118 [trainer.py] => lrate: 0.0005
2025-09-27 17:46:47,118 [trainer.py] => head_scale: 1.0
2025-09-27 17:46:47,118 [trainer.py] => batch_size: 24
2025-09-27 17:46:47,118 [trainer.py] => evaluate_final_only: True
2025-09-27 17:46:47,118 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:46:47,118 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:46:47,118 [trainer.py] => kd_type: feat
2025-09-27 17:46:47,118 [trainer.py] => alpha_t: 1.0
2025-09-27 17:46:47,118 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:46:47,118 [trainer.py] => compensate: True
2025-09-27 17:46:47,118 [trainer.py] => eval_only: False
2025-09-27 17:46:47,118 [trainer.py] => lora_rank: 4
2025-09-27 17:46:47,118 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:46:47,118 [trainer.py] => weight_temp: 5.0
2025-09-27 17:46:47,118 [trainer.py] => weight_kind: log1p
2025-09-27 17:46:47,118 [trainer.py] => weight_p: 1.0
2025-09-27 17:46:47,119 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:46:47,119 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:46:47,119 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:46:47,119 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:46:47,119 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:46:47,119 [trainer.py] => l2_protection: True
2025-09-27 17:46:47,119 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:46:47,119 [trainer.py] => seed: 1993
2025-09-27 17:46:47,119 [trainer.py] => run_id: 0
2025-09-27 17:46:47,119 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:46:48,156 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:46:48,156 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:46:48,163 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:46:48,629 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:46:49,097 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:46:56,627 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:46:56,627 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:46:56,628 [trainer.py] => All params: 86469888
2025-09-27 17:46:56,629 [trainer.py] => Trainable params: 672768
2025-09-27 17:46:57,066 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 17:48:35,056 [trainer.py] => dataset: cifar100
2025-09-27 17:48:35,056 [trainer.py] => smart_defaults: True
2025-09-27 17:48:35,056 [trainer.py] => user: authors
2025-09-27 17:48:35,056 [trainer.py] => test: True
2025-09-27 17:48:35,056 [trainer.py] => memory_size: 0
2025-09-27 17:48:35,056 [trainer.py] => memory_per_class: 0
2025-09-27 17:48:35,056 [trainer.py] => fixed_memory: False
2025-09-27 17:48:35,056 [trainer.py] => shuffle: True
2025-09-27 17:48:35,056 [trainer.py] => init_cls: 20
2025-09-27 17:48:35,057 [trainer.py] => increment: 20
2025-09-27 17:48:35,057 [trainer.py] => model_name: sldc
2025-09-27 17:48:35,057 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:48:35,057 [trainer.py] => weight_decay: 0.0
2025-09-27 17:48:35,057 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:48:35,057 [trainer.py] => sce_a: 0.5
2025-09-27 17:48:35,057 [trainer.py] => sce_b: 0.5
2025-09-27 17:48:35,057 [trainer.py] => seed_list: [1993]
2025-09-27 17:48:35,057 [trainer.py] => iterations: 1500
2025-09-27 17:48:35,058 [trainer.py] => warmup_steps: 200
2025-09-27 17:48:35,058 [trainer.py] => ca_epochs: 5
2025-09-27 17:48:35,058 [trainer.py] => optimizer: adamw
2025-09-27 17:48:35,058 [trainer.py] => lrate: 0.0005
2025-09-27 17:48:35,058 [trainer.py] => head_scale: 1.0
2025-09-27 17:48:35,058 [trainer.py] => batch_size: 24
2025-09-27 17:48:35,058 [trainer.py] => evaluate_final_only: True
2025-09-27 17:48:35,058 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:48:35,058 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:48:35,058 [trainer.py] => kd_type: feat
2025-09-27 17:48:35,058 [trainer.py] => alpha_t: 1.0
2025-09-27 17:48:35,058 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:48:35,058 [trainer.py] => compensate: True
2025-09-27 17:48:35,058 [trainer.py] => eval_only: False
2025-09-27 17:48:35,058 [trainer.py] => lora_rank: 4
2025-09-27 17:48:35,058 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:48:35,058 [trainer.py] => weight_temp: 5.0
2025-09-27 17:48:35,058 [trainer.py] => weight_kind: log1p
2025-09-27 17:48:35,059 [trainer.py] => weight_p: 1.0
2025-09-27 17:48:35,059 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:48:35,059 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:48:35,059 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:48:35,059 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:48:35,059 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:48:35,059 [trainer.py] => l2_protection: True
2025-09-27 17:48:35,059 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:48:35,059 [trainer.py] => seed: 1993
2025-09-27 17:48:35,059 [trainer.py] => run_id: 0
2025-09-27 17:48:35,059 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:48:36,097 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:48:36,097 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:48:36,104 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:48:36,565 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:48:36,994 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:48:44,611 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:48:44,611 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:48:44,612 [trainer.py] => All params: 86469888
2025-09-27 17:48:44,612 [trainer.py] => Trainable params: 672768
2025-09-27 17:48:45,053 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 17:48:48,996 [subspace_lora.py] => step: 1, loss: 1.6656, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0417
2025-09-27 17:48:52,809 [subspace_lora.py] => step: 2, loss: 1.6611, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0417
2025-09-27 17:48:56,815 [subspace_lora.py] => step: 3, loss: 1.6489, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0458
2025-09-27 17:49:00,708 [subspace_lora.py] => step: 4, loss: 1.6340, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0454
2025-09-27 17:49:04,612 [subspace_lora.py] => step: 5, loss: 1.6127, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0534
2025-09-27 17:49:08,495 [subspace_lora.py] => step: 6, loss: 1.6033, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0522
2025-09-27 17:49:12,399 [subspace_lora.py] => step: 7, loss: 1.5965, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0553
2025-09-27 17:49:16,383 [subspace_lora.py] => step: 8, loss: 1.5782, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0665
2025-09-27 17:49:20,327 [subspace_lora.py] => step: 9, loss: 1.5622, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0723
2025-09-27 17:49:24,321 [subspace_lora.py] => step: 10, loss: 1.5424, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0859
2025-09-27 17:49:28,303 [subspace_lora.py] => step: 11, loss: 1.5173, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0940
2025-09-27 17:49:32,373 [subspace_lora.py] => step: 12, loss: 1.5018, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.1096
2025-09-27 17:49:36,338 [subspace_lora.py] => step: 13, loss: 1.4858, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.1320
2025-09-27 17:54:00,723 [trainer.py] => dataset: cifar100
2025-09-27 17:54:00,723 [trainer.py] => smart_defaults: True
2025-09-27 17:54:00,724 [trainer.py] => user: authors
2025-09-27 17:54:00,724 [trainer.py] => test: True
2025-09-27 17:54:00,724 [trainer.py] => memory_size: 0
2025-09-27 17:54:00,724 [trainer.py] => memory_per_class: 0
2025-09-27 17:54:00,724 [trainer.py] => fixed_memory: False
2025-09-27 17:54:00,724 [trainer.py] => shuffle: True
2025-09-27 17:54:00,724 [trainer.py] => init_cls: 20
2025-09-27 17:54:00,724 [trainer.py] => increment: 20
2025-09-27 17:54:00,724 [trainer.py] => model_name: sldc
2025-09-27 17:54:00,724 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:54:00,724 [trainer.py] => weight_decay: 0.0
2025-09-27 17:54:00,724 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:54:00,724 [trainer.py] => sce_a: 0.5
2025-09-27 17:54:00,724 [trainer.py] => sce_b: 0.5
2025-09-27 17:54:00,724 [trainer.py] => seed_list: [1993]
2025-09-27 17:54:00,724 [trainer.py] => iterations: 1500
2025-09-27 17:54:00,724 [trainer.py] => warmup_steps: 200
2025-09-27 17:54:00,724 [trainer.py] => ca_epochs: 5
2025-09-27 17:54:00,724 [trainer.py] => optimizer: adamw
2025-09-27 17:54:00,725 [trainer.py] => lrate: 0.0005
2025-09-27 17:54:00,725 [trainer.py] => head_scale: 1.0
2025-09-27 17:54:00,725 [trainer.py] => batch_size: 24
2025-09-27 17:54:00,725 [trainer.py] => evaluate_final_only: True
2025-09-27 17:54:00,725 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:54:00,725 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:54:00,725 [trainer.py] => kd_type: feat
2025-09-27 17:54:00,725 [trainer.py] => alpha_t: 1.0
2025-09-27 17:54:00,725 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:54:00,725 [trainer.py] => compensate: True
2025-09-27 17:54:00,725 [trainer.py] => eval_only: False
2025-09-27 17:54:00,725 [trainer.py] => lora_rank: 4
2025-09-27 17:54:00,725 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:54:00,725 [trainer.py] => weight_temp: 5.0
2025-09-27 17:54:00,725 [trainer.py] => weight_kind: log1p
2025-09-27 17:54:00,725 [trainer.py] => weight_p: 1.0
2025-09-27 17:54:00,726 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:54:00,726 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:54:00,726 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:54:00,726 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:54:00,726 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:54:00,726 [trainer.py] => l2_protection: True
2025-09-27 17:54:00,726 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:54:00,726 [trainer.py] => seed: 1993
2025-09-27 17:54:00,726 [trainer.py] => run_id: 0
2025-09-27 17:54:00,726 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:54:01,772 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:54:01,773 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:54:01,779 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:54:02,239 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:54:02,704 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:57:26,141 [trainer.py] => dataset: cifar100
2025-09-27 17:57:26,141 [trainer.py] => smart_defaults: True
2025-09-27 17:57:26,141 [trainer.py] => user: authors
2025-09-27 17:57:26,141 [trainer.py] => test: True
2025-09-27 17:57:26,141 [trainer.py] => memory_size: 0
2025-09-27 17:57:26,141 [trainer.py] => memory_per_class: 0
2025-09-27 17:57:26,142 [trainer.py] => fixed_memory: False
2025-09-27 17:57:26,142 [trainer.py] => shuffle: True
2025-09-27 17:57:26,142 [trainer.py] => init_cls: 20
2025-09-27 17:57:26,142 [trainer.py] => increment: 20
2025-09-27 17:57:26,142 [trainer.py] => model_name: sldc
2025-09-27 17:57:26,142 [trainer.py] => vit_type: vit-b-p16
2025-09-27 17:57:26,142 [trainer.py] => weight_decay: 0.0
2025-09-27 17:57:26,142 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 17:57:26,142 [trainer.py] => sce_a: 0.5
2025-09-27 17:57:26,142 [trainer.py] => sce_b: 0.5
2025-09-27 17:57:26,142 [trainer.py] => seed_list: [1993]
2025-09-27 17:57:26,143 [trainer.py] => iterations: 1500
2025-09-27 17:57:26,143 [trainer.py] => warmup_steps: 200
2025-09-27 17:57:26,144 [trainer.py] => ca_epochs: 5
2025-09-27 17:57:26,144 [trainer.py] => optimizer: adamw
2025-09-27 17:57:26,144 [trainer.py] => lrate: 0.0005
2025-09-27 17:57:26,144 [trainer.py] => head_scale: 1.0
2025-09-27 17:57:26,144 [trainer.py] => batch_size: 24
2025-09-27 17:57:26,144 [trainer.py] => evaluate_final_only: True
2025-09-27 17:57:26,144 [trainer.py] => gamma_norm: 0.1
2025-09-27 17:57:26,144 [trainer.py] => gamma_kd: 0.0
2025-09-27 17:57:26,144 [trainer.py] => kd_type: feat
2025-09-27 17:57:26,144 [trainer.py] => alpha_t: 1.0
2025-09-27 17:57:26,144 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 17:57:26,144 [trainer.py] => compensate: True
2025-09-27 17:57:26,144 [trainer.py] => eval_only: False
2025-09-27 17:57:26,144 [trainer.py] => lora_rank: 4
2025-09-27 17:57:26,144 [trainer.py] => lora_type: sgp_lora
2025-09-27 17:57:26,145 [trainer.py] => weight_temp: 5.0
2025-09-27 17:57:26,145 [trainer.py] => weight_kind: log1p
2025-09-27 17:57:26,145 [trainer.py] => weight_p: 1.0
2025-09-27 17:57:26,145 [trainer.py] => nsp_eps: 0.05
2025-09-27 17:57:26,145 [trainer.py] => nsp_weight: 0.0
2025-09-27 17:57:26,145 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 17:57:26,145 [trainer.py] => aux_dataset: imagenet
2025-09-27 17:57:26,145 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 17:57:26,145 [trainer.py] => l2_protection: True
2025-09-27 17:57:26,145 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 17:57:26,145 [trainer.py] => seed: 1993
2025-09-27 17:57:26,145 [trainer.py] => run_id: 0
2025-09-27 17:57:26,145 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 17:57:27,192 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 17:57:27,193 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 17:57:27,199 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 17:57:27,660 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 17:57:28,149 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 17:57:36,085 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 17:57:36,086 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 17:57:36,087 [trainer.py] => All params: 86469888
2025-09-27 17:57:36,087 [trainer.py] => Trainable params: 672768
2025-09-27 17:57:36,654 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 17:57:41,055 [subspace_lora.py] => step: 1, loss: 1.6656, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0417
2025-09-27 18:02:21,944 [trainer.py] => dataset: cifar100
2025-09-27 18:02:21,945 [trainer.py] => smart_defaults: True
2025-09-27 18:02:21,945 [trainer.py] => user: authors
2025-09-27 18:02:21,945 [trainer.py] => test: True
2025-09-27 18:02:21,945 [trainer.py] => memory_size: 0
2025-09-27 18:02:21,945 [trainer.py] => memory_per_class: 0
2025-09-27 18:02:21,945 [trainer.py] => fixed_memory: False
2025-09-27 18:02:21,945 [trainer.py] => shuffle: True
2025-09-27 18:02:21,945 [trainer.py] => init_cls: 20
2025-09-27 18:02:21,945 [trainer.py] => increment: 20
2025-09-27 18:02:21,945 [trainer.py] => model_name: sldc
2025-09-27 18:02:21,945 [trainer.py] => vit_type: vit-b-p16
2025-09-27 18:02:21,946 [trainer.py] => weight_decay: 0.0
2025-09-27 18:02:21,946 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 18:02:21,946 [trainer.py] => sce_a: 0.5
2025-09-27 18:02:21,946 [trainer.py] => sce_b: 0.5
2025-09-27 18:02:21,946 [trainer.py] => seed_list: [1993]
2025-09-27 18:02:21,946 [trainer.py] => iterations: 1500
2025-09-27 18:02:21,946 [trainer.py] => warmup_steps: 200
2025-09-27 18:02:21,946 [trainer.py] => ca_epochs: 5
2025-09-27 18:02:21,946 [trainer.py] => optimizer: adamw
2025-09-27 18:02:21,946 [trainer.py] => lrate: 0.0005
2025-09-27 18:02:21,946 [trainer.py] => head_scale: 1.0
2025-09-27 18:02:21,946 [trainer.py] => batch_size: 24
2025-09-27 18:02:21,946 [trainer.py] => evaluate_final_only: True
2025-09-27 18:02:21,947 [trainer.py] => gamma_norm: 0.1
2025-09-27 18:02:21,947 [trainer.py] => gamma_kd: 0.0
2025-09-27 18:02:21,947 [trainer.py] => kd_type: feat
2025-09-27 18:02:21,947 [trainer.py] => alpha_t: 1.0
2025-09-27 18:02:21,947 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 18:02:21,947 [trainer.py] => compensate: True
2025-09-27 18:02:21,947 [trainer.py] => eval_only: False
2025-09-27 18:02:21,947 [trainer.py] => lora_rank: 4
2025-09-27 18:02:21,947 [trainer.py] => lora_type: sgp_lora
2025-09-27 18:02:21,947 [trainer.py] => weight_temp: 5.0
2025-09-27 18:02:21,947 [trainer.py] => weight_kind: log1p
2025-09-27 18:02:21,947 [trainer.py] => weight_p: 1.0
2025-09-27 18:02:21,947 [trainer.py] => nsp_eps: 0.05
2025-09-27 18:02:21,947 [trainer.py] => nsp_weight: 0.0
2025-09-27 18:02:21,947 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 18:02:21,948 [trainer.py] => aux_dataset: imagenet
2025-09-27 18:02:21,948 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 18:02:21,948 [trainer.py] => l2_protection: True
2025-09-27 18:02:21,948 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 18:02:21,948 [trainer.py] => seed: 1993
2025-09-27 18:02:21,948 [trainer.py] => run_id: 0
2025-09-27 18:02:21,948 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 18:02:23,004 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 18:02:23,005 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 18:02:23,011 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 18:02:23,526 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 18:02:23,933 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 18:02:31,563 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 18:02:31,564 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 18:02:31,564 [trainer.py] => All params: 86469888
2025-09-27 18:02:31,565 [trainer.py] => Trainable params: 672768
2025-09-27 18:02:32,025 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 18:02:36,291 [subspace_lora.py] => step: 1, loss: 1.6656, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0417
2025-09-27 18:02:40,665 [subspace_lora.py] => step: 2, loss: 1.6611, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0417
2025-09-27 18:02:44,950 [subspace_lora.py] => step: 3, loss: 1.6489, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0458
2025-09-27 18:02:48,947 [subspace_lora.py] => step: 4, loss: 1.6340, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0454
2025-09-27 18:03:35,989 [trainer.py] => dataset: cifar100
2025-09-27 18:03:35,989 [trainer.py] => smart_defaults: True
2025-09-27 18:03:35,990 [trainer.py] => user: authors
2025-09-27 18:03:35,990 [trainer.py] => test: True
2025-09-27 18:03:35,990 [trainer.py] => memory_size: 0
2025-09-27 18:03:35,990 [trainer.py] => memory_per_class: 0
2025-09-27 18:03:35,990 [trainer.py] => fixed_memory: False
2025-09-27 18:03:35,990 [trainer.py] => shuffle: True
2025-09-27 18:03:35,990 [trainer.py] => init_cls: 20
2025-09-27 18:03:35,990 [trainer.py] => increment: 20
2025-09-27 18:03:35,990 [trainer.py] => model_name: sldc
2025-09-27 18:03:35,990 [trainer.py] => vit_type: vit-b-p16
2025-09-27 18:03:35,990 [trainer.py] => weight_decay: 0.0
2025-09-27 18:03:35,990 [trainer.py] => device: [device(type='cuda', index=0)]
2025-09-27 18:03:35,990 [trainer.py] => sce_a: 0.5
2025-09-27 18:03:35,990 [trainer.py] => sce_b: 0.5
2025-09-27 18:03:35,990 [trainer.py] => seed_list: [1993]
2025-09-27 18:03:35,990 [trainer.py] => iterations: 1500
2025-09-27 18:03:35,990 [trainer.py] => warmup_steps: 200
2025-09-27 18:03:35,990 [trainer.py] => ca_epochs: 5
2025-09-27 18:03:35,991 [trainer.py] => optimizer: adamw
2025-09-27 18:03:35,991 [trainer.py] => lrate: 0.0005
2025-09-27 18:03:35,991 [trainer.py] => head_scale: 1.0
2025-09-27 18:03:35,991 [trainer.py] => batch_size: 24
2025-09-27 18:03:35,991 [trainer.py] => evaluate_final_only: True
2025-09-27 18:03:35,991 [trainer.py] => gamma_norm: 0.1
2025-09-27 18:03:35,991 [trainer.py] => gamma_kd: 0.0
2025-09-27 18:03:35,991 [trainer.py] => kd_type: feat
2025-09-27 18:03:35,991 [trainer.py] => alpha_t: 1.0
2025-09-27 18:03:35,991 [trainer.py] =>  _gamma_1: 0.0001
2025-09-27 18:03:35,991 [trainer.py] => compensate: True
2025-09-27 18:03:35,991 [trainer.py] => eval_only: False
2025-09-27 18:03:35,991 [trainer.py] => lora_rank: 4
2025-09-27 18:03:35,991 [trainer.py] => lora_type: sgp_lora
2025-09-27 18:03:35,991 [trainer.py] => weight_temp: 5.0
2025-09-27 18:03:35,991 [trainer.py] => weight_kind: log1p
2025-09-27 18:03:35,991 [trainer.py] => weight_p: 1.0
2025-09-27 18:03:35,992 [trainer.py] => nsp_eps: 0.05
2025-09-27 18:03:35,992 [trainer.py] => nsp_weight: 0.0
2025-09-27 18:03:35,992 [trainer.py] => auxiliary_data_path: /data1/open_datasets/ImageNet-2012/train
2025-09-27 18:03:35,992 [trainer.py] => aux_dataset: imagenet
2025-09-27 18:03:35,992 [trainer.py] => auxiliary_data_size: 1024
2025-09-27 18:03:35,992 [trainer.py] => l2_protection: True
2025-09-27 18:03:35,992 [trainer.py] => l2_protection_lambda: 0.0001
2025-09-27 18:03:35,992 [trainer.py] => seed: 1993
2025-09-27 18:03:35,992 [trainer.py] => run_id: 0
2025-09-27 18:03:35,992 [trainer.py] => log_path: sldc_logs_authors\cifar100_vit-b-p16\init-20_inc-20_rank-4_lt-sgp_lora_T-5_comp-1_wk-log1p_wp-1_kd-feat_gkd-0_gn-0p1_optim-adamw_lr-0p0005_bz-24_iter-1500_seed-1993
2025-09-27 18:03:37,049 [data_manager1.py] => [IDM] load dataset cifar100 | train=50000, test=10000, classes=100
2025-09-27 18:03:37,049 [data_manager1.py] => [IDM] class_order: [68, 56, 78, 8, 23, 84, 90, 65, 74, 76, 40, 89, 3, 92, 55, 9, 26, 80, 43, 38, 58, 70, 77, 1, 85, 19, 17, 50, 28, 53, 13, 81, 45, 82, 6, 59, 83, 16, 15, 44, 91, 41, 72, 60, 79, 52, 20, 10, 31, 54, 37, 95, 14, 71, 96, 98, 97, 2, 64, 66, 42, 22, 35, 86, 24, 34, 87, 21, 99, 0, 88, 27, 18, 94, 11, 12, 47, 25, 30, 46, 62, 69, 36, 61, 7, 63, 75, 5, 32, 4, 51, 48, 73, 93, 39, 67, 29, 49, 57, 33]
2025-09-27 18:03:37,056 [data_manager1.py] => [IDM] increment_classess=[20, 20, 20, 20, 20] (nb_tasks=5)
2025-09-27 18:03:37,522 [_builder.py] => Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)
2025-09-27 18:03:38,083 [_hub.py] => [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.
2025-09-27 18:03:47,342 [subspace_lora.py] => Compiled network with torch.compile
2025-09-27 18:03:47,343 [subspace_lora.py] => Optimizer instantiated: lrate=0.0005, wd=0.0, optimizer=adamw
2025-09-27 18:03:47,343 [trainer.py] => All params: 86469888
2025-09-27 18:03:47,344 [trainer.py] => Trainable params: 672768
2025-09-27 18:03:47,555 [subspace_lora.py] => System training on classes 0-20 (cifar100)
2025-09-27 18:03:48,186 [subspace_lora.py] => step: 1, loss: 1.6656, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0417
2025-09-27 18:03:48,482 [subspace_lora.py] => step: 2, loss: 1.6611, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0417
2025-09-27 18:03:48,777 [subspace_lora.py] => step: 3, loss: 1.6489, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0458
2025-09-27 18:03:49,081 [subspace_lora.py] => step: 4, loss: 1.6340, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0454
2025-09-27 18:03:49,385 [subspace_lora.py] => step: 5, loss: 1.6127, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0534
2025-09-27 18:03:49,677 [subspace_lora.py] => step: 6, loss: 1.6033, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0522
2025-09-27 18:03:49,966 [subspace_lora.py] => step: 7, loss: 1.5965, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0553
2025-09-27 18:03:50,256 [subspace_lora.py] => step: 8, loss: 1.5782, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0665
2025-09-27 18:03:50,545 [subspace_lora.py] => step: 9, loss: 1.5622, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0723
2025-09-27 18:03:50,839 [subspace_lora.py] => step: 10, loss: 1.5424, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0859
2025-09-27 18:03:51,132 [subspace_lora.py] => step: 11, loss: 1.5173, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.0940
2025-09-27 18:03:51,425 [subspace_lora.py] => step: 12, loss: 1.5018, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.1096
2025-09-27 18:03:51,719 [subspace_lora.py] => step: 13, loss: 1.4858, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.1320
2025-09-27 18:03:52,011 [subspace_lora.py] => step: 14, loss: 1.4601, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.1479
2025-09-27 18:03:52,299 [subspace_lora.py] => step: 15, loss: 1.4354, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.1665
2025-09-27 18:03:52,595 [subspace_lora.py] => step: 16, loss: 1.4106, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.1790
2025-09-27 18:03:52,885 [subspace_lora.py] => step: 17, loss: 1.3875, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.2153
2025-09-27 18:03:53,178 [subspace_lora.py] => step: 18, loss: 1.3562, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.2562
2025-09-27 18:03:53,470 [subspace_lora.py] => step: 19, loss: 1.3344, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.2764
2025-09-27 18:03:53,759 [subspace_lora.py] => step: 20, loss: 1.3305, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.2738
2025-09-27 18:03:54,059 [subspace_lora.py] => step: 21, loss: 1.3046, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.2923
2025-09-27 18:03:54,359 [subspace_lora.py] => step: 22, loss: 1.2758, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.3255
2025-09-27 18:03:54,652 [subspace_lora.py] => step: 23, loss: 1.2520, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.3430
2025-09-27 18:03:54,947 [subspace_lora.py] => step: 24, loss: 1.2273, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.3670
2025-09-27 18:03:55,238 [subspace_lora.py] => step: 25, loss: 1.2175, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.3720
2025-09-27 18:03:55,529 [subspace_lora.py] => step: 26, loss: 1.1911, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.3806
2025-09-27 18:03:55,822 [subspace_lora.py] => step: 27, loss: 1.1905, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.3717
2025-09-27 18:03:56,116 [subspace_lora.py] => step: 28, loss: 1.1726, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.3720
2025-09-27 18:03:56,406 [subspace_lora.py] => step: 29, loss: 1.1333, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.4098
2025-09-27 18:03:56,698 [subspace_lora.py] => step: 30, loss: 1.1088, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.4314
2025-09-27 18:03:56,987 [subspace_lora.py] => step: 31, loss: 1.0747, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.4591
2025-09-27 18:03:57,275 [subspace_lora.py] => step: 32, loss: 1.0354, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.4881
2025-09-27 18:03:57,568 [subspace_lora.py] => step: 33, loss: 1.0143, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5102
2025-09-27 18:03:57,862 [subspace_lora.py] => step: 34, loss: 0.9948, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5092
2025-09-27 18:03:58,153 [subspace_lora.py] => step: 35, loss: 0.9486, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5374
2025-09-27 18:03:58,445 [subspace_lora.py] => step: 36, loss: 0.9243, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5462
2025-09-27 18:03:58,741 [subspace_lora.py] => step: 37, loss: 0.9138, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5499
2025-09-27 18:03:59,042 [subspace_lora.py] => step: 38, loss: 0.8902, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5574
2025-09-27 18:03:59,349 [subspace_lora.py] => step: 39, loss: 0.8558, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5683
2025-09-27 18:03:59,644 [subspace_lora.py] => step: 40, loss: 0.8358, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5615
2025-09-27 18:03:59,948 [subspace_lora.py] => step: 41, loss: 0.8220, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5720
2025-09-27 18:04:00,236 [subspace_lora.py] => step: 42, loss: 0.7854, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.5898
2025-09-27 18:04:00,555 [subspace_lora.py] => step: 43, loss: 0.7512, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6058
2025-09-27 18:04:00,857 [subspace_lora.py] => step: 44, loss: 0.7290, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6036
2025-09-27 18:04:01,146 [subspace_lora.py] => step: 45, loss: 0.7063, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6224
2025-09-27 18:04:01,432 [subspace_lora.py] => step: 46, loss: 0.6582, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6476
2025-09-27 18:04:01,726 [subspace_lora.py] => step: 47, loss: 0.6478, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6454
2025-09-27 18:04:02,017 [subspace_lora.py] => step: 48, loss: 0.6417, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6433
2025-09-27 18:04:02,315 [subspace_lora.py] => step: 49, loss: 0.6145, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6540
2025-09-27 18:04:02,604 [subspace_lora.py] => step: 50, loss: 0.5862, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6636
2025-09-27 18:04:02,889 [subspace_lora.py] => step: 51, loss: 0.5600, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6764
2025-09-27 18:04:03,184 [subspace_lora.py] => step: 52, loss: 0.5335, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6796
2025-09-27 18:04:03,476 [subspace_lora.py] => step: 53, loss: 0.5306, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6783
2025-09-27 18:04:03,766 [subspace_lora.py] => step: 54, loss: 0.5003, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6938
2025-09-27 18:04:04,071 [subspace_lora.py] => step: 55, loss: 0.4913, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.6911
2025-09-27 18:04:04,371 [subspace_lora.py] => step: 56, loss: 0.4578, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7095
2025-09-27 18:04:04,662 [subspace_lora.py] => step: 57, loss: 0.4328, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7135
2025-09-27 18:04:04,953 [subspace_lora.py] => step: 58, loss: 0.3975, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7255
2025-09-27 18:04:05,240 [subspace_lora.py] => step: 59, loss: 0.3923, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7238
2025-09-27 18:04:05,533 [subspace_lora.py] => step: 60, loss: 0.3769, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7264
2025-09-27 18:04:05,820 [subspace_lora.py] => step: 61, loss: 0.3529, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7329
2025-09-27 18:04:06,108 [subspace_lora.py] => step: 62, loss: 0.3349, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7388
2025-09-27 18:04:06,397 [subspace_lora.py] => step: 63, loss: 0.3224, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7399
2025-09-27 18:04:06,688 [subspace_lora.py] => step: 64, loss: 0.3130, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7451
2025-09-27 18:04:06,975 [subspace_lora.py] => step: 65, loss: 0.3147, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7373
2025-09-27 18:04:07,268 [subspace_lora.py] => step: 66, loss: 0.2786, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7469
2025-09-27 18:04:07,566 [subspace_lora.py] => step: 67, loss: 0.2673, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7430
2025-09-27 18:04:07,856 [subspace_lora.py] => step: 68, loss: 0.2700, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7437
2025-09-27 18:04:08,149 [subspace_lora.py] => step: 69, loss: 0.2446, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7443
2025-09-27 18:04:08,441 [subspace_lora.py] => step: 70, loss: 0.2413, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7532
2025-09-27 18:04:08,730 [subspace_lora.py] => step: 71, loss: 0.2151, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7571
2025-09-27 18:04:09,030 [subspace_lora.py] => step: 72, loss: 0.1932, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7689
2025-09-27 18:04:09,327 [subspace_lora.py] => step: 73, loss: 0.1645, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7753
2025-09-27 18:04:09,615 [subspace_lora.py] => step: 74, loss: 0.1574, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7770
2025-09-27 18:04:09,904 [subspace_lora.py] => step: 75, loss: 0.1479, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7826
2025-09-27 18:04:10,193 [subspace_lora.py] => step: 76, loss: 0.1595, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7793
2025-09-27 18:04:10,481 [subspace_lora.py] => step: 77, loss: 0.1407, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7847
2025-09-27 18:04:10,770 [subspace_lora.py] => step: 78, loss: 0.1244, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7896
2025-09-27 18:04:11,060 [subspace_lora.py] => step: 79, loss: 0.1369, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7898
2025-09-27 18:04:11,348 [subspace_lora.py] => step: 80, loss: 0.1209, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7942
2025-09-27 18:04:11,633 [subspace_lora.py] => step: 81, loss: 0.0817, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8064
2025-09-27 18:04:11,919 [subspace_lora.py] => step: 82, loss: 0.0628, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8049
2025-09-27 18:04:12,207 [subspace_lora.py] => step: 83, loss: 0.0567, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7994
2025-09-27 18:04:12,496 [subspace_lora.py] => step: 84, loss: 0.0506, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8028
2025-09-27 18:04:12,782 [subspace_lora.py] => step: 85, loss: 0.0232, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8142
2025-09-27 18:04:13,075 [subspace_lora.py] => step: 86, loss: 0.0006, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8161
2025-09-27 18:04:13,372 [subspace_lora.py] => step: 87, loss: -0.0153, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8220
2025-09-27 18:04:13,669 [subspace_lora.py] => step: 88, loss: -0.0164, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8273
2025-09-27 18:04:13,964 [subspace_lora.py] => step: 89, loss: 0.0103, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8196
2025-09-27 18:04:14,270 [subspace_lora.py] => step: 90, loss: 0.0122, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8168
2025-09-27 18:04:14,557 [subspace_lora.py] => step: 91, loss: 0.0208, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8143
2025-09-27 18:04:14,844 [subspace_lora.py] => step: 92, loss: 0.0107, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8203
2025-09-27 18:04:15,138 [subspace_lora.py] => step: 93, loss: 0.0456, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7966
2025-09-27 18:04:15,430 [subspace_lora.py] => step: 94, loss: 0.0517, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.7878
2025-09-27 18:04:15,718 [subspace_lora.py] => step: 95, loss: 0.0191, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8007
2025-09-27 18:04:16,010 [subspace_lora.py] => step: 96, loss: 0.0115, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8081
2025-09-27 18:04:16,305 [subspace_lora.py] => step: 97, loss: 0.0032, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8148
2025-09-27 18:04:16,597 [subspace_lora.py] => step: 98, loss: -0.0200, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8250
2025-09-27 18:04:16,884 [subspace_lora.py] => step: 99, loss: -0.0238, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8258
2025-09-27 18:04:17,191 [subspace_lora.py] => step: 100, loss: -0.0269, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8224
2025-09-27 18:04:17,485 [subspace_lora.py] => step: 101, loss: -0.0410, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8277
2025-09-27 18:04:17,786 [subspace_lora.py] => step: 102, loss: -0.0442, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8241
2025-09-27 18:04:18,090 [subspace_lora.py] => step: 103, loss: -0.0628, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8250
2025-09-27 18:04:18,385 [subspace_lora.py] => step: 104, loss: -0.0655, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8258
2025-09-27 18:04:18,675 [subspace_lora.py] => step: 105, loss: -0.0735, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8308
2025-09-27 18:04:18,972 [subspace_lora.py] => step: 106, loss: -0.0871, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8393
2025-09-27 18:04:19,275 [subspace_lora.py] => step: 107, loss: -0.1070, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8387
2025-09-27 18:04:19,567 [subspace_lora.py] => step: 108, loss: -0.1147, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8465
2025-09-27 18:04:19,854 [subspace_lora.py] => step: 109, loss: -0.1065, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8369
2025-09-27 18:04:20,148 [subspace_lora.py] => step: 110, loss: -0.0982, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8324
2025-09-27 18:04:20,436 [subspace_lora.py] => step: 111, loss: -0.1055, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8408
2025-09-27 18:04:20,737 [subspace_lora.py] => step: 112, loss: -0.0895, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8359
2025-09-27 18:04:21,035 [subspace_lora.py] => step: 113, loss: -0.0812, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8273
2025-09-27 18:04:21,329 [subspace_lora.py] => step: 114, loss: -0.0785, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8237
2025-09-27 18:04:21,620 [subspace_lora.py] => step: 115, loss: -0.0695, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8164
2025-09-27 18:04:21,908 [subspace_lora.py] => step: 116, loss: -0.0518, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8056
2025-09-27 18:04:22,199 [subspace_lora.py] => step: 117, loss: -0.0654, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8083
2025-09-27 18:04:22,498 [subspace_lora.py] => step: 118, loss: -0.0927, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8233
2025-09-27 18:04:22,785 [subspace_lora.py] => step: 119, loss: -0.0818, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8160
2025-09-27 18:04:23,072 [subspace_lora.py] => step: 120, loss: -0.0901, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8219
2025-09-27 18:04:23,362 [subspace_lora.py] => step: 121, loss: -0.0693, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8147
2025-09-27 18:04:23,652 [subspace_lora.py] => step: 122, loss: -0.0798, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8207
2025-09-27 18:04:23,944 [subspace_lora.py] => step: 123, loss: -0.0875, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8262
2025-09-27 18:04:24,252 [subspace_lora.py] => step: 124, loss: -0.1055, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8352
2025-09-27 18:04:24,543 [subspace_lora.py] => step: 125, loss: -0.1198, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8475
2025-09-27 18:04:24,832 [subspace_lora.py] => step: 126, loss: -0.0673, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8211
2025-09-27 18:04:25,119 [subspace_lora.py] => step: 127, loss: -0.0571, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8182
2025-09-27 18:04:25,406 [subspace_lora.py] => step: 128, loss: -0.0395, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8113
2025-09-27 18:04:25,696 [subspace_lora.py] => step: 129, loss: -0.0664, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8260
2025-09-27 18:04:25,988 [subspace_lora.py] => step: 130, loss: -0.0756, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8309
2025-09-27 18:04:26,282 [subspace_lora.py] => step: 131, loss: -0.0628, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8228
2025-09-27 18:04:26,575 [subspace_lora.py] => step: 132, loss: -0.0895, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8364
2025-09-27 18:04:26,863 [subspace_lora.py] => step: 133, loss: -0.0857, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8319
2025-09-27 18:04:27,149 [subspace_lora.py] => step: 134, loss: -0.0995, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8362
2025-09-27 18:04:27,437 [subspace_lora.py] => step: 135, loss: -0.0838, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8234
2025-09-27 18:04:27,733 [subspace_lora.py] => step: 136, loss: -0.1000, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8328
2025-09-27 18:04:28,034 [subspace_lora.py] => step: 137, loss: -0.1053, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8370
2025-09-27 18:04:28,340 [subspace_lora.py] => step: 138, loss: -0.0955, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8325
2025-09-27 18:04:28,640 [subspace_lora.py] => step: 139, loss: -0.0890, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8325
2025-09-27 18:04:28,936 [subspace_lora.py] => step: 140, loss: -0.0766, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8285
2025-09-27 18:04:29,244 [subspace_lora.py] => step: 141, loss: -0.0933, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8331
2025-09-27 18:04:29,536 [subspace_lora.py] => step: 142, loss: -0.0936, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8373
2025-09-27 18:04:29,828 [subspace_lora.py] => step: 143, loss: -0.0985, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8411
2025-09-27 18:04:30,117 [subspace_lora.py] => step: 144, loss: -0.0978, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8361
2025-09-27 18:04:30,406 [subspace_lora.py] => step: 145, loss: -0.0996, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8358
2025-09-27 18:04:30,697 [subspace_lora.py] => step: 146, loss: -0.0752, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8314
2025-09-27 18:04:30,986 [subspace_lora.py] => step: 147, loss: -0.0888, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8400
2025-09-27 18:04:31,270 [subspace_lora.py] => step: 148, loss: -0.0871, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8435
2025-09-27 18:04:31,563 [subspace_lora.py] => step: 149, loss: -0.0874, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8466
2025-09-27 18:04:31,850 [subspace_lora.py] => step: 150, loss: -0.1079, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8578
2025-09-27 18:04:32,150 [subspace_lora.py] => step: 151, loss: -0.1266, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8595
2025-09-27 18:04:32,458 [subspace_lora.py] => step: 152, loss: -0.1279, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8569
2025-09-27 18:04:32,757 [subspace_lora.py] => step: 153, loss: -0.1397, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8629
2025-09-27 18:04:33,047 [subspace_lora.py] => step: 154, loss: -0.1639, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8724
2025-09-27 18:04:33,331 [subspace_lora.py] => step: 155, loss: -0.1606, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8768
2025-09-27 18:04:33,613 [subspace_lora.py] => step: 156, loss: -0.1415, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8642
2025-09-27 18:04:33,910 [subspace_lora.py] => step: 157, loss: -0.1483, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8611
2025-09-27 18:04:34,219 [subspace_lora.py] => step: 158, loss: -0.1715, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8708
2025-09-27 18:04:34,509 [subspace_lora.py] => step: 159, loss: -0.1753, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8712
2025-09-27 18:04:34,799 [subspace_lora.py] => step: 160, loss: -0.1819, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8758
2025-09-27 18:04:35,100 [subspace_lora.py] => step: 161, loss: -0.1797, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8715
2025-09-27 18:04:35,394 [subspace_lora.py] => step: 162, loss: -0.1751, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8677
2025-09-27 18:04:35,683 [subspace_lora.py] => step: 163, loss: -0.1868, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8768
2025-09-27 18:04:35,972 [subspace_lora.py] => step: 164, loss: -0.2029, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8849
2025-09-27 18:04:36,259 [subspace_lora.py] => step: 165, loss: -0.2131, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8881
2025-09-27 18:04:36,549 [subspace_lora.py] => step: 166, loss: -0.2131, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8868
2025-09-27 18:04:36,836 [subspace_lora.py] => step: 167, loss: -0.2204, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8898
2025-09-27 18:04:37,124 [subspace_lora.py] => step: 168, loss: -0.2170, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8883
2025-09-27 18:04:37,408 [subspace_lora.py] => step: 169, loss: -0.2162, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8870
2025-09-27 18:04:37,694 [subspace_lora.py] => step: 170, loss: -0.2253, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8941
2025-09-27 18:04:37,980 [subspace_lora.py] => step: 171, loss: -0.2194, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8922
2025-09-27 18:04:38,267 [subspace_lora.py] => step: 172, loss: -0.2230, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8905
2025-09-27 18:04:38,552 [subspace_lora.py] => step: 173, loss: -0.2062, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8806
2025-09-27 18:04:38,843 [subspace_lora.py] => step: 174, loss: -0.1911, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8800
2025-09-27 18:04:39,143 [subspace_lora.py] => step: 175, loss: -0.1858, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8754
2025-09-27 18:04:39,434 [subspace_lora.py] => step: 176, loss: -0.1922, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8795
2025-09-27 18:04:39,724 [subspace_lora.py] => step: 177, loss: -0.1954, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8832
2025-09-27 18:04:40,018 [subspace_lora.py] => step: 178, loss: -0.1907, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8782
2025-09-27 18:04:40,307 [subspace_lora.py] => step: 179, loss: -0.1781, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8737
2025-09-27 18:04:40,597 [subspace_lora.py] => step: 180, loss: -0.1810, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8739
2025-09-27 18:04:40,894 [subspace_lora.py] => step: 181, loss: -0.1786, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8740
2025-09-27 18:04:41,181 [subspace_lora.py] => step: 182, loss: -0.1778, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8741
2025-09-27 18:04:41,466 [subspace_lora.py] => step: 183, loss: -0.1677, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8742
2025-09-27 18:04:41,768 [subspace_lora.py] => step: 184, loss: -0.1516, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8659
2025-09-27 18:04:42,056 [subspace_lora.py] => step: 185, loss: -0.1433, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8543
2025-09-27 18:04:42,345 [subspace_lora.py] => step: 186, loss: -0.1592, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8564
2025-09-27 18:04:42,633 [subspace_lora.py] => step: 187, loss: -0.1871, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8708
2025-09-27 18:04:42,941 [subspace_lora.py] => step: 188, loss: -0.1822, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8670
2025-09-27 18:04:43,237 [subspace_lora.py] => step: 189, loss: -0.1894, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8678
2025-09-27 18:04:43,533 [subspace_lora.py] => step: 190, loss: -0.1907, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8602
2025-09-27 18:04:43,826 [subspace_lora.py] => step: 191, loss: -0.1785, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8575
2025-09-27 18:04:44,131 [subspace_lora.py] => step: 192, loss: -0.1865, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8634
2025-09-27 18:04:44,429 [subspace_lora.py] => step: 193, loss: -0.1894, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8604
2025-09-27 18:04:44,723 [subspace_lora.py] => step: 194, loss: -0.2022, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8702
2025-09-27 18:04:45,011 [subspace_lora.py] => step: 195, loss: -0.1797, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8540
2025-09-27 18:04:45,298 [subspace_lora.py] => step: 196, loss: -0.1772, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8520
2025-09-27 18:04:45,585 [subspace_lora.py] => step: 197, loss: -0.1785, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8543
2025-09-27 18:04:45,875 [subspace_lora.py] => step: 198, loss: -0.1643, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8480
2025-09-27 18:04:46,163 [subspace_lora.py] => step: 199, loss: -0.1666, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8507
2025-09-27 18:04:46,460 [subspace_lora.py] => step: 200, loss: -0.1525, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8448
2025-09-27 18:04:46,749 [subspace_lora.py] => step: 201, loss: -0.1648, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8561
2025-09-27 18:04:47,034 [subspace_lora.py] => step: 202, loss: -0.1671, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8622
2025-09-27 18:04:47,323 [subspace_lora.py] => step: 203, loss: -0.1755, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8635
2025-09-27 18:04:47,611 [subspace_lora.py] => step: 204, loss: -0.1624, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8605
2025-09-27 18:04:47,901 [subspace_lora.py] => step: 205, loss: -0.1624, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8578
2025-09-27 18:04:48,193 [subspace_lora.py] => step: 206, loss: -0.1826, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8678
2025-09-27 18:04:48,480 [subspace_lora.py] => step: 207, loss: -0.2017, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8810
2025-09-27 18:04:48,771 [subspace_lora.py] => step: 208, loss: -0.1926, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8804
2025-09-27 18:04:49,071 [subspace_lora.py] => step: 209, loss: -0.1758, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8757
2025-09-27 18:04:49,366 [subspace_lora.py] => step: 210, loss: -0.1578, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8715
2025-09-27 18:04:49,652 [subspace_lora.py] => step: 211, loss: -0.1702, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8760
2025-09-27 18:04:49,938 [subspace_lora.py] => step: 212, loss: -0.1721, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8801
2025-09-27 18:04:50,223 [subspace_lora.py] => step: 213, loss: -0.1613, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8712
2025-09-27 18:04:50,508 [subspace_lora.py] => step: 214, loss: -0.1812, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8758
2025-09-27 18:04:50,796 [subspace_lora.py] => step: 215, loss: -0.1649, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8674
2025-09-27 18:04:51,083 [subspace_lora.py] => step: 216, loss: -0.1720, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8723
2025-09-27 18:04:51,369 [subspace_lora.py] => step: 217, loss: -0.1754, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8767
2025-09-27 18:04:51,654 [subspace_lora.py] => step: 218, loss: -0.1806, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8766
2025-09-27 18:04:51,941 [subspace_lora.py] => step: 219, loss: -0.1862, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8722
2025-09-27 18:04:52,225 [subspace_lora.py] => step: 220, loss: -0.1801, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8683
2025-09-27 18:04:52,514 [subspace_lora.py] => step: 221, loss: -0.1601, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8607
2025-09-27 18:04:52,800 [subspace_lora.py] => step: 222, loss: -0.1719, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8663
2025-09-27 18:04:53,088 [subspace_lora.py] => step: 223, loss: -0.1578, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8630
2025-09-27 18:04:53,376 [subspace_lora.py] => step: 224, loss: -0.1461, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8558
2025-09-27 18:04:53,662 [subspace_lora.py] => step: 225, loss: -0.1579, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8578
2025-09-27 18:04:53,952 [subspace_lora.py] => step: 226, loss: -0.1586, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8595
2025-09-27 18:04:54,253 [subspace_lora.py] => step: 227, loss: -0.1603, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8569
2025-09-27 18:04:54,539 [subspace_lora.py] => step: 228, loss: -0.1641, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8629
2025-09-27 18:04:54,827 [subspace_lora.py] => step: 229, loss: -0.1684, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8641
2025-09-27 18:04:55,111 [subspace_lora.py] => step: 230, loss: -0.1626, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8610
2025-09-27 18:04:55,398 [subspace_lora.py] => step: 231, loss: -0.1717, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8707
2025-09-27 18:04:55,685 [subspace_lora.py] => step: 232, loss: -0.1807, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8795
2025-09-27 18:04:55,969 [subspace_lora.py] => step: 233, loss: -0.1832, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8790
2025-09-27 18:04:56,256 [subspace_lora.py] => step: 234, loss: -0.1553, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8578
2025-09-27 18:04:56,544 [subspace_lora.py] => step: 235, loss: -0.1618, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8554
2025-09-27 18:04:56,828 [subspace_lora.py] => step: 236, loss: -0.1676, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8615
2025-09-27 18:04:57,116 [subspace_lora.py] => step: 237, loss: -0.1590, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8587
2025-09-27 18:04:57,404 [subspace_lora.py] => step: 238, loss: -0.1555, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8603
2025-09-27 18:04:57,700 [subspace_lora.py] => step: 239, loss: -0.1478, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8618
2025-09-27 18:04:57,990 [subspace_lora.py] => step: 240, loss: -0.1363, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8548
2025-09-27 18:04:58,279 [subspace_lora.py] => step: 241, loss: -0.1434, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8610
2025-09-27 18:04:58,570 [subspace_lora.py] => step: 242, loss: -0.1532, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8665
2025-09-27 18:04:58,858 [subspace_lora.py] => step: 243, loss: -0.1621, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8715
2025-09-27 18:04:59,156 [subspace_lora.py] => step: 244, loss: -0.1551, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8636
2025-09-27 18:04:59,448 [subspace_lora.py] => step: 245, loss: -0.1690, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8730
2025-09-27 18:04:59,734 [subspace_lora.py] => step: 246, loss: -0.1640, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8732
2025-09-27 18:05:00,023 [subspace_lora.py] => step: 247, loss: -0.1680, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8734
2025-09-27 18:05:00,312 [subspace_lora.py] => step: 248, loss: -0.1663, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8777
2025-09-27 18:05:00,596 [subspace_lora.py] => step: 249, loss: -0.1906, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8858
2025-09-27 18:05:00,881 [subspace_lora.py] => step: 250, loss: -0.2016, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8847
2025-09-27 18:05:01,167 [subspace_lora.py] => step: 251, loss: -0.2228, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8921
2025-09-27 18:05:01,450 [subspace_lora.py] => step: 252, loss: -0.2359, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8945
2025-09-27 18:05:01,742 [subspace_lora.py] => step: 253, loss: -0.2046, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8801
2025-09-27 18:05:02,037 [subspace_lora.py] => step: 254, loss: -0.1842, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8754
2025-09-27 18:05:02,327 [subspace_lora.py] => step: 255, loss: -0.2022, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8837
2025-09-27 18:05:02,616 [subspace_lora.py] => step: 256, loss: -0.1852, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8703
2025-09-27 18:05:02,903 [subspace_lora.py] => step: 257, loss: -0.1644, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8583
2025-09-27 18:05:03,188 [subspace_lora.py] => step: 258, loss: -0.1636, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8600
2025-09-27 18:05:03,471 [subspace_lora.py] => step: 259, loss: -0.1784, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8656
2025-09-27 18:05:03,758 [subspace_lora.py] => step: 260, loss: -0.1815, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8582
2025-09-27 18:05:04,051 [subspace_lora.py] => step: 261, loss: -0.1468, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8474
2025-09-27 18:05:04,347 [subspace_lora.py] => step: 262, loss: -0.1514, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8502
2025-09-27 18:05:04,631 [subspace_lora.py] => step: 263, loss: -0.1598, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8485
2025-09-27 18:05:04,914 [subspace_lora.py] => step: 264, loss: -0.1543, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8470
2025-09-27 18:05:05,199 [subspace_lora.py] => step: 265, loss: -0.1529, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8498
2025-09-27 18:05:05,484 [subspace_lora.py] => step: 266, loss: -0.1733, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8606
2025-09-27 18:05:05,767 [subspace_lora.py] => step: 267, loss: -0.1832, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8621
2025-09-27 18:05:06,049 [subspace_lora.py] => step: 268, loss: -0.1750, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8592
2025-09-27 18:05:06,332 [subspace_lora.py] => step: 269, loss: -0.1834, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8608
2025-09-27 18:05:06,619 [subspace_lora.py] => step: 270, loss: -0.1922, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8622
2025-09-27 18:05:06,907 [subspace_lora.py] => step: 271, loss: -0.1823, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8551
2025-09-27 18:05:07,192 [subspace_lora.py] => step: 272, loss: -0.1619, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8446
2025-09-27 18:05:07,481 [subspace_lora.py] => step: 273, loss: -0.1824, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8560
2025-09-27 18:05:07,771 [subspace_lora.py] => step: 274, loss: -0.1960, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8662
2025-09-27 18:05:08,059 [subspace_lora.py] => step: 275, loss: -0.1842, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8671
2025-09-27 18:05:08,345 [subspace_lora.py] => step: 276, loss: -0.1606, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8471
2025-09-27 18:05:08,632 [subspace_lora.py] => step: 277, loss: -0.1801, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8582
2025-09-27 18:05:08,924 [subspace_lora.py] => step: 278, loss: -0.1856, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8557
2025-09-27 18:05:09,228 [subspace_lora.py] => step: 279, loss: -0.1947, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8576
2025-09-27 18:05:09,517 [subspace_lora.py] => step: 280, loss: -0.1646, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8469
2025-09-27 18:05:09,802 [subspace_lora.py] => step: 281, loss: -0.1725, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8539
2025-09-27 18:05:10,087 [subspace_lora.py] => step: 282, loss: -0.1707, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8518
2025-09-27 18:05:10,372 [subspace_lora.py] => step: 283, loss: -0.1633, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8458
2025-09-27 18:05:10,658 [subspace_lora.py] => step: 284, loss: -0.1517, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8404
2025-09-27 18:05:10,942 [subspace_lora.py] => step: 285, loss: -0.1281, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8355
2025-09-27 18:05:11,233 [subspace_lora.py] => step: 286, loss: -0.1355, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8395
2025-09-27 18:05:11,523 [subspace_lora.py] => step: 287, loss: -0.1288, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8347
2025-09-27 18:05:11,807 [subspace_lora.py] => step: 288, loss: -0.1240, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8345
2025-09-27 18:05:12,090 [subspace_lora.py] => step: 289, loss: -0.1147, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8219
2025-09-27 18:05:12,375 [subspace_lora.py] => step: 290, loss: -0.1274, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8356
2025-09-27 18:05:12,664 [subspace_lora.py] => step: 291, loss: -0.1415, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8395
2025-09-27 18:05:12,951 [subspace_lora.py] => step: 292, loss: -0.1555, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8472
2025-09-27 18:05:13,236 [subspace_lora.py] => step: 293, loss: -0.1635, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8500
2025-09-27 18:05:13,523 [subspace_lora.py] => step: 294, loss: -0.1699, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8567
2025-09-27 18:05:13,810 [subspace_lora.py] => step: 295, loss: -0.1779, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8627
2025-09-27 18:05:14,106 [subspace_lora.py] => step: 296, loss: -0.1844, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8639
2025-09-27 18:05:14,396 [subspace_lora.py] => step: 297, loss: -0.1933, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8692
2025-09-27 18:05:14,682 [subspace_lora.py] => step: 298, loss: -0.2122, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8823
2025-09-27 18:05:14,971 [subspace_lora.py] => step: 299, loss: -0.2180, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8815
2025-09-27 18:05:15,256 [subspace_lora.py] => step: 300, loss: -0.2019, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8809
2025-09-27 18:05:15,541 [subspace_lora.py] => step: 301, loss: -0.2119, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8845
2025-09-27 18:05:15,834 [subspace_lora.py] => step: 302, loss: -0.2118, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8793
2025-09-27 18:05:16,126 [subspace_lora.py] => step: 303, loss: -0.2305, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8914
2025-09-27 18:05:16,418 [subspace_lora.py] => step: 304, loss: -0.2314, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8939
2025-09-27 18:05:16,702 [subspace_lora.py] => step: 305, loss: -0.2145, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8879
2025-09-27 18:05:16,992 [subspace_lora.py] => step: 306, loss: -0.2303, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8991
2025-09-27 18:05:17,277 [subspace_lora.py] => step: 307, loss: -0.2208, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8967
2025-09-27 18:05:17,570 [subspace_lora.py] => step: 308, loss: -0.2282, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8987
2025-09-27 18:05:17,859 [subspace_lora.py] => step: 309, loss: -0.2291, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8963
2025-09-27 18:05:18,146 [subspace_lora.py] => step: 310, loss: -0.2220, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8900
2025-09-27 18:05:18,437 [subspace_lora.py] => step: 311, loss: -0.2254, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8843
2025-09-27 18:05:18,725 [subspace_lora.py] => step: 312, loss: -0.2268, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8834
2025-09-27 18:05:19,025 [subspace_lora.py] => step: 313, loss: -0.2236, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8826
2025-09-27 18:05:19,330 [subspace_lora.py] => step: 314, loss: -0.2279, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8776
2025-09-27 18:05:19,635 [subspace_lora.py] => step: 315, loss: -0.2406, kd_loss: 0.0000, prior_loss: 0.0000, acc: 0.8857
